---
title: '[Data] 빅데이터 - 하둡, 하이브로 시작하기'
date: 2025-03-08 18:10:00 +0900
categories: [빅데이터, Hadoop]
tags: [Hadoop]
math: true
mermaid: true
---

## 빅데이터
### 5V 특징
- Volume - 크기
- Variety - 다양성
- Velocity - 속도
- Value - 가치
- Veracity - 정확성

### 수집 형태
- 정형 : 데이터베이스, CSV, 엑셀과 같이 칼럼 단위의 명확한 구분자와 형태가 존재하는 데이터
- 반정형 : XML, HTML, JSON 형태와 같이 여러 가지 형태가 있을 수 있지만, 메타데이터나 스키마가 존재하는 데이터
- 비정형 " 동영상, SNS 메시지, 사진, 오디오, 음성 데이터처럼 형태가 존재하지 않는 데이터

### 수집 시간
- 배치 : 시, 일, 주, 월 단위로 일정한 주기로 수집, 처리되는 데이터
- 실시간 : 실시간 검색어, 실시간 차트 처럼 사용자의 입력과 동시에 처리되는 데이터

### 처리 단계
- 수집
- 정제
- 적재
- 분석
- 시각화

### 데이터 수집 기술
1. 수집
- Flume : 플룸은 많은 양의 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어
- Kafka : 오픈 소스 메시지 브로커 프로젝트
- Sqoop : 관계형 데이터 베이스와 아파치 하둡간의 대용량 데이터들을 효율적으로 변환 하여 주는 명령 줄 인터페이스 애플리케이션
- Nifi : 소프트웨어 시스템 간 데이터 흐름을 자동화하도록 설계된 소프트웨어 프로젝트
- Flink : 오픈 소스 스트림 처리 프레임 워크
- Splunk : 기계가 생성한 빅 데이터를, 웹 스타일 인터페이스를 통해 검색, 모니터링, 분석하는 소프트웨어
- Logstash : 실시간 파이프라인 기능을 가진 오픈소스 데이터 수집 엔진
- Fluentd : 크로스 플랫폼 오픈 소스 데이터 수집 소프트웨어 프로젝트

2. 정제
- Identification : 알려진 다양한 데이터 포맷이나 비정형 데이터에 할당된 기본 포맷을 식별
- Filtration : 수집된 정보에서 정확하지 않은 데이터는 제외
- Validation : 데이터 유효성을 검증
- Noise Reduction : 오류 데이터를 제거, 분석 불가능한 데이터는 제외
- Transformation : 데이터를 분석 가능한 형태로 변환
- Compression : 저장장치 효율성을 위해 변환한 데이터를 압축
- Integration : 처리 완료한 데이터를 적재

### 빅데이터 에코시스템
1. 수집 기술 : 빅데이터 분석을 위한 원천 데이터(로그 데이터, DB 데이터, API 호출 데이터)를 수집하는 기술
    - 플룸(Flume) : 클라우데라에서 개발한 서버 로그 수집 도구 입니다. 각 서버에 에이전트가 설치 되고, 에이전트로부터 데이터를 전달 받는 콜렉터로 구성
        - Event, Agent, Source, Channel, Sink
    - 카프카(Kafka) : 링크드인에서 개발한 분산 메시징 시스템입니다. 대용량 실시간 로그 처리에 특화 되어 있습니다. 발행(publish) - 구독(subscribe) 모델로 구성
        - Topic, Partition, Producer, Consumer, Broker
    - NiFi
    - Sqoop : RDBMS와 HDFS간 대용량 데이터 전송을 위한 솔루션
    - scribe
    - Fluentd
    - Hudson
2. 작업 관리 기술 : 빅데이터를 분석하는 여러가지 단계를 효율적으로 생성, 관리하고 모니터링 할 수 있게 도와주는 기술
    - Airflow : 에어비앤비에서 개발한 데이터 흐름의 시각화, 스케쥴링, 모니터링이 가능한 워크플로우 플랫폼입니다. 하이브, 프레스토, DBMS 엔진과 결합하여 사용
    - Azkaban
    - Oozie
3. 데이터 직렬화 : 각 언어간에 내부 객체를 공유해야 하는 경우가 있습니다. 이를 효율적으로 처리하기 위해서 데이터 직렬화기술
    - Avro
    - Thrift
    - Protocol Buffers
4. 저장
    - HDFS : 하둡 분산 파일 시스템(HDFS, Hadoop distributed file system), 하둡 프레임워크를 위해 자바 언어로 작성된 분산 확장 파일 시스템입니다. HDFS는 범용 컴퓨터를 클러스터로 구성하여 대용량의 파일을 블록단위로 분할하여 여러서버에 복제하여 저장
    - S3 : 아마존에서 제공하는 인터넷용 저장소
5. NoSQL
    - HBase : HBase는 HDFS 기반의 칼럼 기반 NoSQL 데이터베이스, HBase의 기본 동작단위는 칼럼. H마스터가 H리전을 관리하는 구조를 가지고 있습니다. 주키퍼가 H마스터를 관리하여 SPOF를 회피
6. 데이터 처리
    - MapReduce : HDFS상에서 동작하는 가장 기본적인 분석 기술. 간단한 단위작업을 반복할 때 효율적인 맵리듀스 모델을 이용하여 데이터를 분석
    - Spark : 인메모리 기반의 범용 데이터 처리 플랫폼. 배치 처리, 머신러닝, SQL 질의 처리, 스트리밍 데이터 처리, 그래프 라이브러리 처리와 같은 다양한 작업을 수용할 수 있도록 설계
    - Impala
    - Presto
    - Hive : 하둡 기반의 데이터웨어하우징용 솔루션
    - Hcatalog
    - Pig
7. 클러스터 관리
    - YARN : 데이터 처리 작업을 실행하기 위한 클러스터 자원(CPU, 메모리, 디스크등)과 스케쥴링을 위한 프레임워크
    - Mesos
8. 분산 서버 관리 : 클러스터에서 여러가지 기술이 이용될 때 하나의 서버에서 모든 작업이 진행되면 이 서버가 단일실패지점(SPOF) 발생. 이로 인한 리스크를 줄이기 위해 분산 서버 관리 기술
    - Zookeeper : 첫째, 하나의 서버에만 서비스가 집중되지 않게 서비스를 알맞게 분산해 동시에 처리. 둘째, 하나의 서버에서 처리한 결과를 다른 서버와도 동기화해서 데이터의 안정성을 보장. 셋째, 운영(active) 서버에 문제가 발생해서 서비스를 제공할 수 없을 경우, 다른 대기 중인 서버를 운영 서버로 바꿔서 서비스가 중지 없이 제공. 넷째, 분산 환경을 구성하는 서버의 환경설정을 통합적으로 관리.
9. 시각화
    - Zeppelin
    - Hue
    - Tableu
10. 보안
    - Ranger
11. 데이터 거버넌스 : 기업의 여기저기 산재한 데이터를 같은 저장소에 관리, 비정형 데이터를 규칙에 맞게 표준화하는 전사 차원의 빅데이터 관리 체계
    - Atlas
    - Amundsen
12. 기타
    - Elastic Search
    - Grafana
    - Prometheus

### 데이터 분석
1. 데이터 구조
    - 데이터 아키텍처
        - 조직이 데이터를 수집, 저장, 처리, 관리하는 방법을 설계하고 구현하는 일련의 과정
    - 데이터 원천
        - 데이터베이스
        - 애플리케이션의 로그, 매출 데이터
        - 웹로그
    - ETL
        - 추출: 많은 데이터중 필요한 데이터를 추출
        - 변환: 분석할 수 있는 형태로 변환, 필요 없는 데이터 제거
        - 로드: DW, DL 등에 저장
    
2. 데이터 추출
    - 데이터 웨어하우스
        - 구조화된 데이터 저장
        - 테이블 형태의 데이터 저장
        - 긴 시간동안 안정적으로 데이터를 저장하고 즉시 조회 가능
    - 데이터 레이크
        - 정형, 반정형, 비정형 데이터 저장
            - 로그파일, 소셜 미디어 데이터, 영상 데이터 등
    - 데이터 마트
        - 특정 부서나 목적에 맞는 데이터
        - 데이터 웨어하우스의 일부 데이터
            - 효율적인 데이터 접근
            - 맞춤형 데이터
            - 보안성

## 하둡(Hadoop)
### 구성 요소
- Hadoop Common : 하둡의 다른 모듈을 지원하기 위한 공통 컴포넌트 모듈
- Hadoop HDFS : 분산저장을 처리하기 위한 모듈. 여러개의 서버를 하나의 서버처럼 묶어서 데이터를 저장
- Hadoop YARN : 병렬처리를 위한 클러스터 자원관리 및 스케줄링 담당
- Hadoop Mapreduce : 분산되어 저장된 데이터를 병렬 처리할 수 있게 해주는 분산 처리 모듈
- Hadoop Ozone : 하둡을 위한 오브젝트 저장소

### 장단점
- 장점
    - 오픈소스로 라이선스에 대한 비용 부담이 적음
    - 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
    - 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음(Fault tolerance)
    - 저렴한 구축 비용과 비용대비 빠른 데이터 처리
    - 오프라인 배치 프로세싱에 최적화
- 단점
    - HDFS에 저장된 데이터를 변경 불가
    - 실시간 데이터 분석 같이 신속하게 처리해야 하는 작업에는 부적합
    - 너무 많은 버전과 부실한 서포트
    - 설정의 어려움

## HDFS
### 특징
- 블록 단위 저장, 블록의 기본 복제 단위는 3
- 마스터 슬레이브 구조로 하나의 네임노드와 여러 개의 데이터노드로 구성.
- 네임노드
    - 메타데이터 관리
        - Fsimage 파일
        - Edits 파일
    - 데이터 노드 관리
        - 데이터노드가 주기적으로 전달하는 하트비트(3초, dfs.heartbeat.interval)와 블록리포트(6시간, dfs.blockreport.intervalMsec)를 이용하여 데이터 노드의 동작상태, 블록상태를 관리
        - 블록리포트를 이용하여 HDFS에 저장된 파일에 대한 최신 정보를 유지. 블록리포트에는 데이터노드에 저장된 블록 목록과 각 볼록이 로컬 디스크의 어디에 저장되어 있는지에 대한 정보를 가지고 있다.
- 데이터 노드
    - 블록 단위로 파일을 저장하는 역할
    - 활성 상태 : 데이터노드가 Live 상태인지 Dead 상태인지 나타냄
    - 운영 상태 : 데이터노드의 업그레이드, 패치 같은 작업을 하기 위해 서비스를 잠시 멈추어야 할 경우 블록을 안전하게 보관하기 위해 설정
- 네임노드 구동 과정
    - Fsimage를 읽어 메모리에 적재합니다.
    - Edits 파일을 읽어와서 변경내역을 반영
    - 현재의 메모리 상태를 스냅샷으로 생성하여 Fsimage 파일 생성
    - 데이터 노드로부터 블록리포트를 수신하여 매핑정보 생성
    - 서비스 시작
- 파일 읽기/쓰기
    - 파일 읽기
        - 네임노드에 파일이 보관된 블록 위치 요청
        - 네임노드가 블록 위치 반환
        - 각 데이터 노드에 파일 블록을 요청
        - 노드의 블록이 깨져 있으면 네임노드에 이를 통지하고 다른 블록 확인
    - 파일 쓰기
        - 네임노드에 파일 정보를 전송하고, 파일의 블록을 써야할 노드 목록 요청
        - 네임노드가 파일을 저장할 목록 반환
        - 데이터 노드에 파일 쓰기 요청
        - 데이터 노드간 복제가 진행

### 구조
- 블록
    - 블록 작업 순서
        - 같은 노드에 있는 데이터
        - 같은 랙(Rack)의 노드에 있는 데이터
        - 다른 랙의 노드에 있는 데이터
- 세컨더리 네임노드
    - Fsimage와 Edits 파일을 주기적으로 머지하여 최신 블록의 상태로 파일을 생성. 파일을 머지하면서 Edits 파일을 삭제하기 때문에 디스크 부족 문제도 해결할 수 있다.

### HDFS Federation
- 네임노드는 파일 정보 메타데이터를 메모리에서 관리합니다. 파일이 많아지면 메모리 사용량이 늘어나게 되고, 메모리 관리가 문제가 됩니다. 이를 해결하기 위해 하둡 v2 부터 HDFS 페더레이션을 지원합니다.
- HDFS 페더레이션은 디렉토리(네임스페이스) 단위로 네임노드를 등록하여 사용하는 것입니다. 예를 들어 user, hadoop, tmp 세개의 디렉토리가 존재할 때, /user, /hadoop, /tmp 디렉토리 단위로 총 3개의 네임노드를 실행하여 파일을 관리하게 하는 것입니다.

### HDFS HA
- HDFS는 네임노드가 단일 실패 지점(SPOF)입니다. 네임노드에 문제가 발생하면 모든 작업이 중지되고, 파일을 읽거나 쓸수 없게 됩니다. 하둡 v2에서 이 문제를 해결하기 위해서 HDFS 고가용성(High Availability)을 제공합니다.
- HDFS 고가용성은 이중화된 두대의 서버인 액티브(active) 네임노드와 스탠바이(standby) 네임노드를 이용하여 지원합니다. 액티브 네임노드와 스탠바이 네임노드는 데이터 노드로부터 블록 리포트와 하트비트를 모두 받아서 동일한 메타데이터를 유지하고, 공유 스토리지나 저널 노드를 이용하여 에디트파일을 공유합니다.
- 액티브 네임노드는 네임노드의 역활을 수행하고, 스탠바이 네임노드는 액티브 네임노드와 동일한 메타데이터 정보를 유지하다가, 액티브 네임노드에 문제가 발생하면 스탠바이 네임노드가 액티브 네임노드로 동작하게 됩니다. 액티브 네임노드에 문제가 발생하는 것을 자동으로 확인하는 것이 어렵기 때문에 보통 주키퍼를 이용하여 장애 발생시 자동으로 변경될 수 있도록 합니다.
- 스탠바이 네임노드는 세컨더리 네임노드의 역할을 동일하게 수행합니다. 따라서 HDFS를 고가용성 모드로 설정하였을 때는 세컨더리 네임노드를 실행하지 않아도 됩니다. 고가용성 모드에서 세컨더리 네임노드를 실행하면 오류가 발생합니다.
- ZKFC(Zookeeper Failover Controller) : HDFS 전용 구현체로, 고가용성 에디트 로그를 지원하기 위한 목적으로 설계되었고 HDFS의 권장 옵션

### HDFS SafeMode
- HDFS의 세이프모드(safemode)는 데이터 노드를 수정할 수 없는 상태 입니다. 세이프 모드가 되면 데이터는 읽기 전용 상태가 되고, 데이터 추가와 수정이 불가능 하며 데이터 복제도 일어나지 않습니다.

### HDFS 데이터 블록 관리
- HDFS 운영중 데이터 노드에 문제가 생기면, 데이터 블록에 문제가 발생 할 수 있습니다. 커럽트(CORRUPT) 상태와 복제 개수 부족(Under replicated) 상태 입니다.

### HDFS 휴지통
- HDFS는 사용자의 실수에 의한 파일 삭제를 방지하기 위해서 휴지통 기능을 제공합니다.
- 휴지통 기능이 설정되면 HDFS에서 삭제한 파일은 바로 삭제되지 않고, 각 사용자의 홈디렉토리 아래 휴지통 디렉토리(/user/유저명/.Trash)로 이동됩니다. 휴지통 아래의 파일은 복구할 수 있습니다.
- 휴지통 디렉토리는 지정한 간격으로 체크포인트가 생성되고, 유효 기간이 만료되면 체크포인트를 삭제합니다. 삭제 되면 해당 블록을 해제하고, 사용자에게 반환합니다.

### HDFS 명령어
- 사용자 커맨드
    - classpath
    - dfs : cat, text, ls, mkdir, cp, mv, get, put, rm setrep, test, touchz, stat, setfacl, getfacl, count
    - fsck : path
- 운영자 커맨드
    - namenode
    - datanode
    - secondarynamenode
    - fsck
    - dfsadmin : -report, -safemode, triggerBlockReport
    - haadmin
    - fechdt
    - checknative
- 디버그 커맨드

### WebHDFS REST API
- HDFS는 REST API를 이용하여 파일을 조회하고, 생성, 수정, 삭제하는 기능을 제공합니다. 이 기능을 이용하여 원격지에서 HDFS의 내용에 접근하는 것이 가능

### HDFS 암호화
- HDFS는 민감정보의 보안을 위해 암호화 기능을 제공1합니다. 암호화를 적용하면 디스크에 저장되는 파일을 암호화하여 저장하고, HDFS의 디렉토리에 접근할 때 하둡 KMS를 이용하여 키기반으로 전송 데이터의 암/복호화를 지원합니다.
- 하둡 KMS2는 REST API 서버도 제공합니다. 9700 포트가 기본포트

### RPC
- HDFS는 서버와 클라이언트간 통신에 RPC를 이용합니다. RPC는 Remote Procedure Call은 원격지에 있는 노드의 함수를 실행하여 결과를 반환받을 수 있습니다.

### Eraser Coding(EC)
- HDFS는 3x 복제(원본 1개, 복제본 2개, 총 3개의 파일 유지)를 이용해서 내결함성을 제공합니다. 복제는 저장 공간, 네트워크 대역폭 등에서 200%의 오베헤드를 가지는 것으로 계산됩니다. 상대적으로 낮은 액세스 활동이 있는 콜드 데이터 세트의 경우 추가 블록 복제본은 거의 액세스 되지 않지만 다른 복제본과 동일한 양의 리소스를 소비합니다.
- EC(Erasure Coding)를 이용해서 이런 문제를 해결할 수 있습니다. EC는 훨씬 적은 저장 공간을 사용하여 동일한 수준의 내결함성을 제공합니다. 일반적인 EC 설정에서 스토리지 오버헤드는 50% 이하입니다. EC는 파일의 복제 개수는 항상 1이며 setrep 명령을 통해 변경할 수 없습니다.

### Rack Awareness
- Hadoop 구성 요소는 랙을 인식합니다. 예를 들어 HDFS 블록 배치는 하나의 블록 복제본을 서로 다른 랙에 배치하는 내결함성을 위해 랙 인식을 사용합니다. 이는 클러스터 내에서 네트워크 스위치 장애 또는 파티션이 발생한 경우 데이터 가용성을 제공합니다.

### 밸런서(Balance)
- HDFS를 운영하면서 데이터 불균형이 발생하여 밸런싱을 실행해야 하는 경우가 있습니다. 주의할 점은 밸런서는 랙 인식(Rack Awareness) 설정이 되어 있지 않으면 동작하지 않습니다.
- 데이터 불균형이 발생하는 경우
    - 데이터 노드를 추가하는 경우
        - 하둡의 데이터 저장 공간이 부족하여 데이터노드를 추가하는 경우 다른 노드의 사용공간은 70~80% 정도인데 신규 데이터 노드는 사용공간이 0%
    - 대량의 데이터를 삭제하는 경우
        - 특정 데이터 노드에 블록이 많이 저장되어 데이터노드간 저장공간 차이가 20~30% 정도 발생하는 경우
    - 대량의 데이터를 추가하는 경우
        - 특정 데이터 노드에 데이터가 적은 경우 네임노드는 데이터 저장공간이 작은 노드를 우선적으로 사용하는데 이 경우 특정 노드로 I/O가 집중 되게 됨

### 권한과 ACL
- HDFS 는 권한 설정과 ACL을 이용하여 파일에 대한 사용자의 접근을 제어할 수 있습니다.

### HDFS 사용량 제한 설정
- 파일 개수 제한
- 파일 용량 제한
- 제한 설정 명령
- 제한 명령 확인

### 데이터 압축
- 데이터 압축을 고려해야 할 가장 중요한 두 가지 장소는 MapReduce 작업 및 HBase에 저장된 데이터 측면입니다.
- GZIP, BZip2

## MapReduce
- 맵리듀스는 간단한 단위작업을 반복하여 처리할 때 사용하는 프로그래밍 모델입니다. 간단한 단위작업을 처리하는 맵(Map) 작업과 맵 작업의 결과물을 모아서 집계하는 리듀스(Reduce) 단계로 구성됩니다.
- 하둡에서 분산처리를 담당하는 맵리듀스 작업은 맵과 리듀스로 나누어져 처리됩니다. 맵, 리듀스작업은 병렬로 처리가 가능한 작업으로, 여러 컴퓨터에서 동시에 작업을 처리하여 속도를 높일 수 있습니다.
- 하둡 v1의 작업 단위는 잡(job)이고, 하둡 v2의 작업 단위는 애플리케이션(application) 입니다. YARN 아키텍처가 도입되면서 이름은 변경되었지만 동일하게 관리됩니다. 잡은 맵 태스크와 리듀스 태스크로 나누어 집니다. 태스크는 어템프트(attempt) 단위로 실행됩니다.
- 맵리듀스는 실행 중 오류가 발생하면 설정1된 횟수만큼 자동으로 반복됩니다. 반복후에도 오류가 발생하면 작업을 종료합니다.
- 맵의 입력은 스플릿(InputSplit)단위로 분할됩니다. 맵작업은 큰 데이터를 하나의 노드에서 처리하지 않고, 분할하여 동시에 병렬 처리하여 작업 시간을 단축합니다.
- 스플릿이 작으면 작업 부하가 분산되어 성능을 높을 수 있습니다. 하지만 스플릿의 크기가 너무 작으면 맵 작업의 개수가 증가하고 맵 작업 생성을 위한 오버헤드가 증가하여 작업이 느려질 수 있습니다. 따라서 작업에 따라 적절한 개수의 맵 작업을 생성해야 합니다. 일반적으로 맵 작업의 적절한 스플릿 크기는 데이터 지역성의 이점을 얻을 수 있는 HDFS 블록의 기본 크기(128MB)입니다.
- 맵 작업은 HDFS에 입력 데이터가 있는 노드에서 실행할 때 가장 빠르게 동작합니다. 클러스터의 네트워크 대역을 사용하지 않고 처리할 수 있기 때문입니다. 데이터가 있는 노드에서 작업을 처리할 수 없다면 동일한 랙의 노드, 다른 랙의 노드 순서로 맵 작업이 실행가능한 노드를 찾습니다.
- 맵 작업의 적절한 스플릿 크기가 HDFS 블록의 기본크기인 이유는 단일 노드에 해당 블록이 모두 저장된다고 확신할 수 있는 입력 크기이기 때문입니다. 스플릿 크기가 블록의 기본 크기일때 맵 작업이 로컬 디스크의 데이터만 이용하여, 다른 노드에서 데이터를 전송받아 처리할 때 보다 빠르게 작업을 처리할 수 있습니다.
- 맵 작업의 결과는 로컬 디스크에 임시 저장 됩니다. 맵 작업의 결과는 리듀스 작업의 입력으로 쓰이는 임시 결과물이기 때문입니다. 리듀스 작업은 맵 작업의 결과를 입력으로 받기 때문에 지역성의 장점이 없습니다. 리듀스 작업의 결과는 HDFS에 저장됩니다.
- 리듀스 작업의 개수는 입력 크기와 상관없이 지정할 수 있습니다. 리듀스가 여러개이면 리듀스의 개수 만큼 파티션을 생성하고 맵의 결과를 각 파티션에 분배합니다. 파티션별로 키가 존재하고 동일한 키는 같은 파티션에 전달됩니다.
- 맵리듀스는 리듀서 작업이 있는 경우와 없는 경우가 있습니다. 파일을 읽어서 바로 쓰는 작업의 경우 리듀서가 필요 없어서 매퍼만 있는 작업(Mapper Only)이 됩니다. 집계를 진행해야 해서 리듀서가 필요한 경우 정렬이 필요한 경우는 리듀서가 하나만 생성됩니다. 나머지의 경우 리듀서가 여러개로 생성됩니다. 각 작업의 최종 매퍼, 리듀서의 수만큼 파일이 생성됩니다.

### 처리 단계
1. 입력 : 데이터를 입력하는 단계. 텍스트, csv, gzip 형태의 데이터를 읽어서 맵으로 전달(InputFormat, InputSplit, RecordReader)
2. 맵(Map) : 입력을 분할하여 키별로 데이터를 처리(Mapper)
3. 컴바이너(Combiner) : 네트워크를 타고 넘어가는 데이터를 줄이기 위하여 맵의 결과를 정리. 로컬 리듀서라고도 함. 컴바이너는 작업의 설정에 따라 없을 수도 있음
4. 파티셔너(Partitoner) : 맵의 출력 결과 키 값을 해쉬 처리하여 어떤 리듀서로 넘길지를 결정
5. 셔플(Shuffle) : 각 리듀서로 데이터 이동
6. 정렬(Sort) : 리듀서로 전달된 데이터를 키 값 기준으로 정렬
7. 리듀서(Reduce) : 리듀서로 데이터를 처리하고 결과를 저장
8. 출력 : 리듀서의 결과를 정의된 형태로 저장(OutputFormat, RecordWriter)

### 보조 도구
- 카운터 : 잡은 기본적으로 맵리듀스의 작업상황 입출력 상황을 확인할 수 있는 카운터를 제공
- 분산 캐쉬 : 맵리듀스 잡에서 공유되는 데이터를 이용해야 할 때 분산 캐쉬를 사용. 맵리듀스 잡에서 데이터를 조인해야 할 경우 이용
- 옵션 파서

### 메모리 설정
- 맵리듀스의 메모리 설정은 mapred-site.xml 파일을 수정하여 변경
- Mapper, Reducer, MR 엔진, TEZ 엔진, 셔플 메모리 설정

### 성능 최적화
- Mapper, Recuder 수 설정
- 정렬 속성 튜닝(io.sort.* 튜닝)
- 컴바이너 클래스 적용
- 맵출력 데이터 압축
- 작은 파일 문제(small file problem) 수정

## YARN
- 하둡2에서 도입한 클러스터 리소스 관리 및 애플리케이션 라이프 사이클 관리를 위한 아키텍처
- 잡트래커의 기능을 분리하여 자원 관리는 리소스 매니저와 노드매니저, 애플리케이션 라이프 사이클 관리 기능은 애플리케이션 마스터와 컨테이너가 담당

### YARN Scheduler
- 피포(FIFO) 스케줄러
- 페어(Fair) 스케줄러(yarn-site.xml, fair-scheduler.xml)
- 커패시티(Capacity) 스케줄러(capacity-scheduler.xml) : 커패시티 스케줄러는 트리 형태로 계층화된 큐를 선언하고, 큐별로 사용가능한 용량을 할당하여 자원을 관리합니다. 예를 들어 100G의 메모리 용량을 가지는 클러스터에서 A, B 두개의 큐에 각각 40%, 60%의 용량(capacity)를 설정하면 A큐는 40G, B큐는 60G의 메모리를 사용할 수 있습니다.

### 메모리 설정
- yarn-site.xml 파일을 수정하여 변경
- 노드매니저의 메모리, CPU 개수와 컨테이너에 할당 할 수 있는 최대, 최소 메모리등을 설정

### YARN 명령어
- 사용자 커맨드 : application, container, logs
- 운영자 커맨드 : rmadmin

### YARN REST API
- 리소스 매니저는 REST API를 제공합니다. 이를 통해 클러스터의 상태정보, 운영정보를 확인할 수 있습니다. 응답형태는 JSON, XML 형태로 제공됩니다.

### YARN Node Labels
- 서버를 특성에 맞게 구분하여 작업을 처리하게 하는 기능을 제공합니다.

### YARN HA
- YARN은 리소스 매니저가 단일 실패 지점입니다. 리소스 매니저에 문제가 발생하면 클러스터의 자원관리, 작업 관리 기능을 사용할 수 없기 때문에 하둡 2.4 버전부터 HA 기능을 제공합니다.

### 타임라인 서비스
- 많은 경우 사용자는 작업의 진행 흐름(flow) 또는 YARN 응용 프로그램의 논리적 그룹 수준의 정보에 관심이 있습니다. 논리적 애플리케이션을 완료하기 위해 일련의 YARN 애플리케이션을 시작하는 것이 훨씬 더 일반적입니다. 타임 라인 서비스 v.2는 흐름 개념을 명시 적으로 지원합니다. 또한 흐름 수준에서 메트릭 집계를 지원합니다.
- 또한 구성 및 메트릭과 같은 정보는 가장 중요한 정보로 취급됩니다. 다음 다이어그램은 서로 다른 YARN 항목 모델링 흐름 간의 관계를 보여줍니다.

### 작업 지원 도구(Hadoop Common)
- DistCp : 클러스터내의 대규모 데이터 이동을 위한 DistCp(Distribute Copy) 기능을 제공
- 하둡 아카이브 : 은 사이즈의 파일이 많아지면 네임노드에서 이를 관리하는데 많은 어려움을 겪게 되는 문제가 발생합니다. 따라서 블록사이즈 정도로 파일을 유지해주는 것이 좋습니다. 이를 위해서 하둡은 파일을 묶어서 관리하고, 사용할 수 있는 하둡 아카이브(Hadoop Archive) 기능을 제공
- 하둡 성능 테스트 도구

### 오브젝트 저장소(Hadoop Ozone)
- 오존은 볼륨, 버켓, 키로 구성됩니다. 볼륨은 사용자 계정과 유사합니다. 관리자만 볼륨을 생성하거나 삭제할 수 있습니다. 버켓은 디렉터리와 유사합니다. 버켓은 여러 개의 키를 저장할 수 있지만, 다른 버켓은 저장할 수 없습니다. 키는 파일과 유사합니다. 버켓은 여러 개의 키를 저장할 수 있습니다. 볼륨 > 버켓 > 키 단위로 구성됩니다.

### 설정
- 하둡 관련 주요 설정은 ${HADOOP_HOME}/conf 아래 설정합니다.
- core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml, hadoop-env.sh

## 하이브(Hive)
- 하둡 에코시스템 중에서 데이터를 모델링하고 프로세싱하는 경우 가장 많이 사용하는 데이터 웨어하우징용 솔루션
- RDB의 데이터베이스, 테이블과 같은 형태로 HDFS에 저장된 데이터의 구조를 정의하는 방법을 제공하며, 이 데이터를 대상으로 SQL과 유사한 HiveQL 쿼리를 이용하여 데이터를 조회하는 방법을 제공

## 하둡 클러스터(Hadoop Cluster)

## 참고
- <https://wikidocs.net/book/2203>